{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.functions import monotonically_increasing_id, row_number, desc\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import IntegerType, TimestampType, StructType, StructField, StringType, DateType, BooleanType, DecimalType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('creds.cfg'))\n",
    "\n",
    "KEY             = config.get('AWS','KEY')\n",
    "SECRET          = config.get('AWS','SECRET')\n",
    "\n",
    "pd.DataFrame({\"Param\":\n",
    "                  [\"KEY\", \"SECRET\"],\n",
    "              \"Value\":\n",
    "                  [KEY, SECRET] })\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('creds.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['KEY']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['SECRET']\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .config(\"spark.executor.instances\", 10) \\\n",
    "        .config(\"spark.executor.memory\", \"8g\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "The goal of this project is to provide an analytics team with access to a data model that allows them to easily query and extract insights. To achieve this, I am downloading CSV files, processing them with Python and Spark, and uploading the resulting data to an S3 bucket in the form of Parquet files. These Parquet files can be queried like tables, reading from S3, making it easy for the analytics team to perform various types of analysis.\n",
    "\n",
    "Some examples of specific analyses that can be performed with this data model include:\n",
    "\n",
    "1. Aggregate calculations: Use SQL queries to compute sums, averages, counts, and other aggregate values for various dimensions of the data.\n",
    "\n",
    "2. Time series analysis: Examine trends and patterns over time by querying the data using time-based filters.\n",
    "\n",
    "3. Correlation analysis: Identify relationships between different variables in the data by computing correlations and performing regression analyses.\n",
    "\n",
    "4. Segmentation: Divide the data into groups based on common characteristics and analyze each group separately to identify trends and patterns within each segment.\n",
    "\n",
    "5. Predictive modeling: Use machine learning algorithms to build models that can predict future outcomes based on the data, e.g. classification algorithms.\n",
    "\n",
    "Overall, this data model will enable the analytics BI team to gain a deeper understanding of the data and uncover valuable insights that will inform and drive business decisions. \n",
    "\n",
    "#### Describe and Gather Data \n",
    "The data set I am using has been gathered from this website (http://insideairbnb.com/get-the-data/) and contains various csv files with information about listing of the properties in airbnb, neighbourhood, and reviews left by visitors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "calendar_csv = 'airbnb_data/calendar.csv'\n",
    "list_det_csv = 'airbnb_data/listings_detailed.csv'\n",
    "list_csv = 'airbnb_data/listings.csv'\n",
    "hoods_csv = 'airbnb_data/neighbourhoods.csv'\n",
    "reviews_csv = 'airbnb_data/reviews_detailed.csv'\n",
    "\n",
    "##pandas df for data exploration\n",
    "calendar_df = pd.read_csv(calendar_csv)\n",
    "list_det_df = pd.read_csv(list_det_csv)\n",
    "list_df = pd.read_csv(list_csv)\n",
    "hoods_df = pd.read_csv(hoods_csv)\n",
    "reviews_df = pd.read_csv(reviews_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "I turned the csv into pd dataframes first to explore and display the data in a nice format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "1. visualise the data in a pandas dataframe to assess quality by eye\n",
    "2. check for nulls, possibly delete nulls or replace with '' or 0 depending on column type\n",
    "3. check for duplciates on key column like unique identifiers, drop the rows if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           listing_id        date available requested_price adjusted_price  \\\n",
      "0  652868795892201022  2022-09-15         f          $85.00         $85.00   \n",
      "1  652868795892201022  2022-09-16         f          $85.00         $85.00   \n",
      "\n",
      "   minimum_nights  maximum_nights  \n",
      "0             1.0          1125.0  \n",
      "1             1.0          1125.0  \n",
      "   listing_id      id        date  reviewer_id reviewer_name  \\\n",
      "0        3176    4283  2009-06-20        21475         Milan   \n",
      "1       22438  218181  2011-04-05       401483     Alexandre   \n",
      "\n",
      "                                            comments  \n",
      "0  excellent stay, i would highly recommend it. a...  \n",
      "1  Javier gave us quite a fright when none of his...  \n",
      "                   id                                               name  \\\n",
      "0  652868795892201022  Kleine Auszeit? Oder Business-Trip? Alles mögl...   \n",
      "1            27080612  Apartment with Living/Sleeping Room & own Kitchen   \n",
      "\n",
      "     host_id    host_name     neighbourhood_group neighbourhood   latitude  \\\n",
      "0   21708794  Familie Sek  Tempelhof - Schöneberg   Lichtenrade  52.357652   \n",
      "1  130216168        Tommy   Marzahn - Hellersdorf     Mahlsdorf  52.520060   \n",
      "\n",
      "   longitude        room_type  price  minimum_nights  number_of_reviews  \\\n",
      "0  13.399098  Entire home/apt     88               1                  0   \n",
      "1  13.659560  Entire home/apt     60               2                126   \n",
      "\n",
      "  last_review  reviews_per_month  calculated_host_listings_count  \\\n",
      "0         NaN                NaN                               1   \n",
      "1  2022-09-11               2.54                               2   \n",
      "\n",
      "   availability_365  number_of_reviews_ltm license  \n",
      "0                 6                      0     NaN  \n",
      "1               163                     18     NaN  \n",
      "                   id                                      listing_url  \\\n",
      "0  652868795892201022  https://www.airbnb.com/rooms/652868795892201022   \n",
      "1            29077694            https://www.airbnb.com/rooms/29077694   \n",
      "\n",
      "        scrape_id last_scraped           source  \\\n",
      "0  20220915162225   2022-09-15      city scrape   \n",
      "1  20220915162225   2022-09-16  previous scrape   \n",
      "\n",
      "                                                name  \\\n",
      "0  Kleine Auszeit? Oder Business-Trip? Alles mögl...   \n",
      "1             Wohnung im Grünen nah an der Metropole   \n",
      "\n",
      "                                         description  \\\n",
      "0  Hallo ihr Lieben,<br /><br />kommt und verbrin...   \n",
      "1  Wohnung befindet sich im Souterrain eines Einf...   \n",
      "\n",
      "                               neighborhood_overview  \\\n",
      "0                                                NaN   \n",
      "1  We live in a  green, quite and save area with ...   \n",
      "\n",
      "                                         picture_url    host_id  \\\n",
      "0  https://a0.muscache.com/pictures/miso/Hosting-...   21708794   \n",
      "1  https://a0.muscache.com/pictures/e2642fca-3833...  219116245   \n",
      "\n",
      "         ...        review_scores_communication review_scores_location  \\\n",
      "0        ...                                NaN                    NaN   \n",
      "1        ...                               4.93                   4.52   \n",
      "\n",
      "  review_scores_value license instant_bookable calculated_host_listings_count  \\\n",
      "0                 NaN     NaN                t                              1   \n",
      "1                4.66     NaN                f                              1   \n",
      "\n",
      "  calculated_host_listings_count_entire_homes  \\\n",
      "0                                           1   \n",
      "1                                           1   \n",
      "\n",
      "  calculated_host_listings_count_private_rooms  \\\n",
      "0                                            0   \n",
      "1                                            0   \n",
      "\n",
      "  calculated_host_listings_count_shared_rooms reviews_per_month  \n",
      "0                                           0               NaN  \n",
      "1                                           0              0.63  \n",
      "\n",
      "[2 rows x 72 columns]\n",
      "    neighbourhood_group        neighbourhood\n",
      "0  Charlottenburg-Wilm.            Barstraße\n",
      "1  Charlottenburg-Wilm.  Charlottenburg Nord\n"
     ]
    }
   ],
   "source": [
    "#rename price at it is also found in another future table, view calendar df\n",
    "calendar_df = calendar_df.rename(columns={'price': 'requested_price'}) #doing this retroactively as I realised could be an issue later on\n",
    "print(calendar_df.head(2))\n",
    "\n",
    "#view review df\n",
    "print(reviews_df.head(2))\n",
    "\n",
    "#view listing df\n",
    "print(list_df.head(2))\n",
    "\n",
    "#drop min and max nights as they appear in another future table, view listing detailed df\n",
    "to_drop = ['maximum_nights','minimum_nights','price']\n",
    "list_det_df = list_det_df.drop(to_drop, axis=1)\n",
    "print(list_det_df.head(2))\n",
    "\n",
    "#view neighbourhoods df\n",
    "print(hoods_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "At this point I already want to merge listing with listing detailed to create one df,\n",
    "I do this below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#merging the two listing df to create one, left join on the detailed\n",
    "listing_df = pd.merge(list_det_df, list_df,  how='left', left_on=['id'], right_on = ['id'])\n",
    "\n",
    "# Transpose the dataframe and drop duplicate columns\n",
    "listing_df_transposed = listing_df.T\n",
    "listing_df_unique = listing_df_transposed.drop_duplicates()\n",
    "\n",
    "# Transpose the dataframe back to its original shape\n",
    "listing_df = listing_df_unique.T\n",
    "\n",
    "#remove the suffix _x generated by column duplication\n",
    "suffix = '_x'\n",
    "listing_df = listing_df.rename(columns={col: col.replace(suffix, '') for col in listing_df.columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#cleaning the columns further in order to cast correct data types in order to convert from pd to spark df\n",
    "listing_df = listing_df.fillna('')\n",
    "\n",
    "listing_df['price'] = listing_df['price'].replace('$', '')\n",
    "null_to_zero = ['bedrooms','beds','review_scores_rating','review_scores_accuracy','review_scores_cleanliness',\n",
    "                'review_scores_checkin','reviews_per_month','review_scores_communication','review_scores_location','review_scores_value']\n",
    "for i in null_to_zero:\n",
    "    listing_df[i] = listing_df[i].replace('', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Check for missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id - 0.0%\n",
      "listing_url - 0.0%\n",
      "scrape_id - 0.0%\n",
      "last_scraped - 0.0%\n",
      "source - 0.0%\n",
      "name - 0.0%\n",
      "description - 0.0%\n",
      "neighborhood_overview - 0.0%\n",
      "picture_url - 0.0%\n",
      "host_id - 0.0%\n",
      "host_url - 0.0%\n",
      "host_name - 0.0%\n",
      "host_since - 0.0%\n",
      "host_location - 0.0%\n",
      "host_about - 0.0%\n",
      "host_response_time - 0.0%\n",
      "host_response_rate - 0.0%\n",
      "host_acceptance_rate - 0.0%\n",
      "host_is_superhost - 0.0%\n",
      "host_thumbnail_url - 0.0%\n",
      "host_picture_url - 0.0%\n",
      "host_neighbourhood - 0.0%\n",
      "host_listings_count - 0.0%\n",
      "host_total_listings_count - 0.0%\n",
      "host_verifications - 0.0%\n",
      "host_has_profile_pic - 0.0%\n",
      "host_identity_verified - 0.0%\n",
      "neighbourhood - 0.0%\n",
      "neighbourhood_cleansed - 0.0%\n",
      "neighbourhood_group_cleansed - 0.0%\n",
      "latitude - 0.0%\n",
      "longitude - 0.0%\n",
      "property_type - 0.0%\n",
      "room_type - 0.0%\n",
      "accommodates - 0.0%\n",
      "bathrooms - 0.0%\n",
      "bathrooms_text - 0.0%\n",
      "bedrooms - 0.0%\n",
      "beds - 0.0%\n",
      "amenities - 0.0%\n",
      "minimum_minimum_nights - 0.0%\n",
      "maximum_minimum_nights - 0.0%\n",
      "minimum_maximum_nights - 0.0%\n",
      "maximum_maximum_nights - 0.0%\n",
      "minimum_nights_avg_ntm - 0.0%\n",
      "maximum_nights_avg_ntm - 0.0%\n",
      "has_availability - 0.0%\n",
      "availability_30 - 0.0%\n",
      "availability_60 - 0.0%\n",
      "availability_90 - 0.0%\n",
      "availability_365 - 0.0%\n",
      "number_of_reviews - 0.0%\n",
      "number_of_reviews_ltm - 0.0%\n",
      "number_of_reviews_l30d - 0.0%\n",
      "first_review - 0.0%\n",
      "last_review - 0.0%\n",
      "review_scores_rating - 0.0%\n",
      "review_scores_accuracy - 0.0%\n",
      "review_scores_cleanliness - 0.0%\n",
      "review_scores_checkin - 0.0%\n",
      "review_scores_communication - 0.0%\n",
      "review_scores_location - 0.0%\n",
      "review_scores_value - 0.0%\n",
      "license - 0.0%\n",
      "instant_bookable - 0.0%\n",
      "calculated_host_listings_count - 0.0%\n",
      "calculated_host_listings_count_entire_homes - 0.0%\n",
      "calculated_host_listings_count_private_rooms - 0.0%\n",
      "calculated_host_listings_count_shared_rooms - 0.0%\n",
      "reviews_per_month - 0.0%\n",
      "name_y - 0.0%\n",
      "price - 0.0%\n",
      "minimum_nights - 0.0%\n",
      "listing_id - 0.0%\n",
      "date - 0.0%\n",
      "available - 0.0%\n",
      "requested_price - 0.0%\n",
      "adjusted_price - 0.0%\n",
      "minimum_nights - 0.0%\n",
      "maximum_nights - 0.0%\n",
      "listing_id - 0.0%\n",
      "id - 0.0%\n",
      "date - 0.0%\n",
      "reviewer_id - 0.0%\n",
      "reviewer_name - 0.0%\n",
      "comments - 0.0%\n",
      "neighbourhood_group - 1.0%\n",
      "neighbourhood - 1.0%\n"
     ]
    }
   ],
   "source": [
    "dfs = [listing_df,calendar_df,reviews_df,hoods_df]\n",
    "\n",
    "# % of missing values per column\n",
    "for df in dfs:\n",
    "    for col in df.columns:\n",
    "        pct_missing = np.mean(df[col].isnull())\n",
    "        print('{} - {}%'.format(col, round(pct_missing*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#dropping bathroom as it is always empty\n",
    "listing_df = listing_df.drop('bathrooms',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Check for duplicates now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate listing_ids in listing:  0\n",
      "Number of duplicate ids in review:  0\n",
      "Number of duplicate neighbourhood_group in hoods_df:  0\n"
     ]
    }
   ],
   "source": [
    "duplicate_listing_id = listing_df[listing_df.duplicated(['id'])]['id'].count()\n",
    "print(\"Number of duplicate listing_ids in listing: \", duplicate_listing_id)\n",
    "\n",
    "#makes no sense to check this for calendar\n",
    "# duplicate_calendar_listing_id = calendar_df[calendar_df.duplicated(['listing_id'])]['listing_id'].count()\n",
    "# print(\"Number of duplicate listing_ids in calendar: \", duplicate_calendar_listing_id)\n",
    "\n",
    "duplicate_review_id = reviews_df[reviews_df.duplicated(['id'])]['id'].count()\n",
    "print(\"Number of duplicate ids in review: \", duplicate_review_id)\n",
    "\n",
    "duplicate_neighbourhood_group = hoods_df[hoods_df.duplicated(['neighbourhood'])]['neighbourhood'].count()\n",
    "print(\"Number of duplicate neighbourhood_group in hoods_df: \", duplicate_neighbourhood_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "No duplicates on key columns, moving on.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Star Schema would be the model of choice as I believe for the way the data is presented and already available, it can very well be tranformed into dimension tables without the need to normalise any further. Additionally I am planning to build a central fact table by joining calendar, listing and reviews to create a source to retrieve events. \n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "I am currently using Spark, so I will be parsing and querying the data on the fly in the notebook without building tables and filling them (CREATE & INSERT/COPY statements).\n",
    "This is a great advantage of using Spark to do this job.\n",
    "Once the fields to keep in the dimension tables and the structure of the fact table is clear, I will upload the files in parquet format in an S3 bucket.\n",
    "From there I will redownload/read the parquet files directly into spark dataframes and query directly for demonstration purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Below I am passing the credentials to access the AWS services, specifically I will use an S3 bucket as \"data lake\",\n",
    "and configuring the environment.\n",
    "\n",
    "I will then initiate the Spark builder to run the Spark job. Note that I requested additional calculating power to the Spark builder,\n",
    "as these files are relatively heavy to upload/download to S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Below I define the schema so that I can apply it to the file when reading it.\n",
    "Disclaimer, it was not working for all the files so I had to manually modify the data types of the fields to transform a pandas df into a spark df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#forcing the schema onto the dataframes as spark reads all fields as string\n",
    "\n",
    "listing_schema = StructType([\n",
    "        StructField('id', IntegerType()),\n",
    "        StructField('listing_url', StringType()),\n",
    "        StructField('scrape_id', IntegerType()),\n",
    "        StructField('last_scraped', TimestampType()),\n",
    "        StructField('source', StringType()),\n",
    "        StructField('name', StringType()),\n",
    "        StructField('description', StringType()),\n",
    "        StructField('neighborhood_overview', StringType()),\n",
    "        StructField('picture_url', StringType()),\n",
    "        StructField('host_id', IntegerType()),\n",
    "        StructField('host_url', StringType()),\n",
    "        StructField('host_name', StringType()),\n",
    "        StructField('host_since', TimestampType()),\n",
    "        StructField('host_location', StringType()),\n",
    "        StructField('host_about', StringType()),\n",
    "        StructField('host_response_time', IntegerType()),\n",
    "        StructField('host_response_rate', IntegerType()),\n",
    "        StructField('host_acceptance_rate', IntegerType()),\n",
    "        StructField('host_is_superhost', StringType()),\n",
    "        StructField('host_thumbnail_url', StringType()),\n",
    "        StructField('host_picture_url', StringType()),\n",
    "        StructField('host_neighbourhood', StringType()),\n",
    "        StructField('host_listings_count', IntegerType()),\n",
    "        StructField('host_total_listings_count', IntegerType()),\n",
    "        StructField('host_verifications', StringType()),\n",
    "        StructField('host_has_profile_pic', StringType()),\n",
    "        StructField('host_identity_verified', StringType()),\n",
    "        StructField('neighbourhood', StringType()),\n",
    "        StructField('neighbourhood_cleansed', StringType()),\n",
    "        StructField('neighbourhood_group_cleansed', StringType()),\n",
    "        StructField('latitude', DoubleType()),\n",
    "        StructField('longitude', DoubleType()),\n",
    "        StructField('property_type', StringType()),\n",
    "        StructField('room_type', StringType()),\n",
    "        StructField('accommodates', StringType()),\n",
    "        #StructField('bathrooms', StringType()),\n",
    "        StructField('bathrooms_text', StringType()),\n",
    "        StructField('bedrooms', IntegerType()),\n",
    "        StructField('beds', IntegerType()),\n",
    "        StructField('amenities', StringType()),\n",
    "        StructField('price', DoubleType()),\n",
    "        StructField('minimum_nights', IntegerType()),\n",
    "        StructField('maximum_nights', IntegerType()),\n",
    "        StructField('minimum_minimum_nights', IntegerType()),\n",
    "        StructField('maximum_minimum_nights', IntegerType()),\n",
    "        StructField('minimum_maximum_nights', IntegerType()),\n",
    "        StructField('maximum_maximum_nights', IntegerType()),\n",
    "        StructField('minimum_nights_avg_ntm', IntegerType()),\n",
    "        StructField('maximum_nights_avg_ntm', IntegerType()),\n",
    "        #StructField('calendar_updated', TimestampType()),\n",
    "        StructField('has_availability', StringType()),\n",
    "        StructField('availability_30', IntegerType()),\n",
    "        StructField('availability_60', IntegerType()),\n",
    "        StructField('availability_90', IntegerType()),\n",
    "        StructField('availability_365', IntegerType()),\n",
    "        StructField('calendar_last_scraped', StringType()),\n",
    "        StructField('number_of_reviews', IntegerType()),\n",
    "        StructField('number_of_reviews_ltm', IntegerType()),\n",
    "        StructField('number_of_reviews_l30d', IntegerType()),\n",
    "        StructField('first_review', TimestampType()),\n",
    "        StructField('last_review', TimestampType()),\n",
    "        StructField('review_scores_rating', IntegerType()),\n",
    "        StructField('review_scores_accuracy', IntegerType()),\n",
    "        StructField('review_scores_cleanliness', IntegerType()),\n",
    "        StructField('review_scores_checkin', IntegerType()),\n",
    "        StructField('review_scores_communication', IntegerType()),\n",
    "        StructField('review_scores_location', IntegerType()),\n",
    "        StructField('review_scores_value', IntegerType()),\n",
    "        StructField('license', StringType()),\n",
    "        StructField('instant_bookable', StringType()),\n",
    "        StructField('calculated_host_listings_count', IntegerType()),\n",
    "        StructField('calculated_host_listings_count_entire_homes', IntegerType()),\n",
    "        StructField('calculated_host_listings_count_private_rooms', IntegerType()),\n",
    "        StructField('calculated_host_listings_count_shared_rooms', IntegerType()),\n",
    "        StructField('reviews_per_month', IntegerType())\n",
    "    ])\n",
    "\n",
    "calendar_schema = StructType([\n",
    "        StructField('listing_id', StringType(), True),\n",
    "        StructField('date', TimestampType(), True),\n",
    "        StructField('available', BooleanType(), True),\n",
    "        StructField('requested_price', DecimalType(), True),\n",
    "        StructField('adjusted_price', DecimalType(), True)\n",
    "    ])\n",
    "\n",
    "hoods_schema = StructType([\n",
    "        StructField('neighbourhood_group', StringType()),\n",
    "        StructField('neighbourhood', StringType())\n",
    "    ])\n",
    "\n",
    "reviews_schema = StructType([\n",
    "        StructField('listing_id', IntegerType()),\n",
    "        StructField('id', IntegerType()),\n",
    "        StructField('date', TimestampType()),\n",
    "        StructField('reviewer_id', IntegerType()),\n",
    "        StructField('reviewer_name', StringType()),\n",
    "        StructField('comments', StringType())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "the code below works better already inferring the schema, still doesn't work for listing.\n",
    "For listing I have merged at the DF level already above, I need to create the spark dataframe from the pandas df instead of the csv.\n",
    "Then recheck the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "listing_df['id'] = listing_df['id'].astype('int32')\n",
    "listing_df['listing_url'] = listing_df['listing_url'].astype('str')\n",
    "listing_df['scrape_id'] = listing_df['scrape_id'].astype('int32')\n",
    "listing_df['last_scraped'] = pd.to_datetime(listing_df['last_scraped'])\n",
    "listing_df['source'] = listing_df['source'].astype('str')\n",
    "listing_df['name'] = listing_df['name'].astype('str')\n",
    "listing_df['description'] = listing_df['description'].astype('str')\n",
    "listing_df['neighborhood_overview'] = listing_df['neighborhood_overview'].astype('str')\n",
    "listing_df['picture_url'] = listing_df['picture_url'].astype('str')\n",
    "listing_df['host_id'] = listing_df['host_id'].astype('int32')\n",
    "listing_df['host_url'] = listing_df['host_url'].astype('str')\n",
    "listing_df['host_name'] = listing_df['host_name'].astype('str')\n",
    "listing_df['host_since'] = pd.to_datetime(listing_df['host_since'])\n",
    "listing_df['host_location'] = listing_df['host_location'].astype('str')\n",
    "listing_df['host_about'] = listing_df['host_about'].astype('str')\n",
    "listing_df['host_response_time'] = listing_df['host_response_time'].astype('str')\n",
    "listing_df['host_response_rate'] = listing_df['host_response_rate'].astype('str')\n",
    "listing_df['host_acceptance_rate'] = listing_df['host_acceptance_rate'].astype('str')\n",
    "listing_df['host_is_superhost'] = listing_df['host_is_superhost'].astype('str')\n",
    "listing_df['host_thumbnail_url'] = listing_df['host_thumbnail_url'].astype('str')\n",
    "listing_df['host_picture_url'] = listing_df['host_picture_url'].astype('str')\n",
    "listing_df['host_neighbourhood'] = listing_df['host_neighbourhood'].astype('str')\n",
    "listing_df['host_listings_count'] = listing_df['host_listings_count'].astype('str')\n",
    "listing_df['host_total_listings_count'] = listing_df['host_total_listings_count'].astype('str')\n",
    "listing_df['host_verifications'] = listing_df['host_verifications'].astype('str')\n",
    "listing_df['host_has_profile_pic'] = listing_df['host_has_profile_pic'].astype('str')\n",
    "listing_df['host_identity_verified'] = listing_df['host_identity_verified'].astype('str')\n",
    "listing_df['neighbourhood'] = listing_df['neighbourhood'].astype('str')\n",
    "listing_df['neighbourhood_cleansed'] = listing_df['neighbourhood_cleansed'].astype('str')\n",
    "listing_df['neighbourhood_group_cleansed'] = listing_df['neighbourhood_group_cleansed'].astype('str')\n",
    "listing_df['latitude'] = listing_df['latitude'].astype('float')\n",
    "listing_df['longitude'] = listing_df['longitude'].astype('float')\n",
    "listing_df['property_type'] = listing_df['property_type'].astype('str')\n",
    "listing_df['room_type'] = listing_df['room_type'].astype('str')\n",
    "listing_df['accommodates'] = listing_df['accommodates'].astype('int32')\n",
    "listing_df['bathrooms_text'] = listing_df['bathrooms_text'].astype('str')\n",
    "listing_df['bedrooms'] = listing_df['bedrooms'].astype('str')\n",
    "listing_df['beds'] = listing_df['beds'].astype('int32')\n",
    "listing_df['amenities'] = listing_df['amenities'].astype('str')\n",
    "listing_df['price'] = listing_df['price'].replace('$', '', regex=True).astype('float', errors='ignore')\n",
    "# listing_df['minimum_nights'] = listing_df['minimum_nights'].astype('int32')\n",
    "# listing_df['maximum_nights'] = listing_df['maximum_nights'].astype('int32')\n",
    "listing_df['minimum_minimum_nights'] = listing_df['minimum_minimum_nights'].astype('int32')\n",
    "listing_df['maximum_minimum_nights'] = listing_df['maximum_minimum_nights'].astype('int32')\n",
    "listing_df['minimum_maximum_nights'] = listing_df['minimum_maximum_nights'].astype('int32')\n",
    "listing_df['maximum_maximum_nights'] = listing_df['maximum_maximum_nights'].astype('int32')\n",
    "listing_df['minimum_nights_avg_ntm'] = listing_df['minimum_nights_avg_ntm'].astype('int32')\n",
    "listing_df['maximum_nights_avg_ntm'] = listing_df['maximum_nights_avg_ntm'].astype('int32')\n",
    "listing_df['has_availability'] = listing_df['has_availability'].astype('str')\n",
    "listing_df['availability_30'] = listing_df['availability_30'].astype('int32')\n",
    "listing_df['availability_60'] = listing_df['availability_60'].astype('int32')\n",
    "listing_df['availability_90'] = listing_df['availability_90'].astype('int32')\n",
    "listing_df['availability_365'] = listing_df['availability_365'].astype('int32')\n",
    "#listing_df['calendar_last_scraped'] = pd.to_datetime(listing_df['calendar_last_scraped'])\n",
    "listing_df['number_of_reviews'] = listing_df['number_of_reviews'].astype('int32')\n",
    "listing_df['number_of_reviews_ltm'] = listing_df['number_of_reviews_ltm'].astype('int32')\n",
    "listing_df['number_of_reviews_l30d'] = listing_df['number_of_reviews_l30d'].astype('int32')\n",
    "listing_df['first_review'] = pd.to_datetime(listing_df['first_review'])\n",
    "listing_df['last_review'] = pd.to_datetime(listing_df['last_review'])\n",
    "listing_df['review_scores_rating'] = listing_df['review_scores_rating'].astype('int32')\n",
    "listing_df['review_scores_accuracy'] = listing_df['review_scores_accuracy'].astype('int32')\n",
    "listing_df['review_scores_cleanliness'] = listing_df['review_scores_cleanliness'].astype('int32')\n",
    "listing_df['review_scores_checkin'] = listing_df['review_scores_checkin'].astype('int32')\n",
    "listing_df['review_scores_communication'] = listing_df['review_scores_communication'].astype('int32')\n",
    "listing_df['review_scores_location'] = listing_df['review_scores_location'].astype('int32')\n",
    "listing_df['review_scores_value'] = listing_df['review_scores_value'].astype('int32')\n",
    "#listing_df['requires_license'] = listing_df['requires_license'].astype('str')\n",
    "listing_df['license'] = listing_df['license'].astype('str')\n",
    "#listing_df['jurisdiction_names'] = listing_df['jurisdiction_names'].astype('str')\n",
    "#listing_df['cancellation_policy'] = listing_df['cancellation_policy'].astype('str')\n",
    "#listing_df['is_business_travel_ready'] = listing_df['is_business_travel_ready'].astype('str')\n",
    "listing_df['instant_bookable'] = listing_df['instant_bookable'].astype('str')\n",
    "#listing_df['require_guest_profile_picture'] = listing_df['require_guest_profile_picture'].astype('str')\n",
    "#listing_df['require_guest_phone_verification'] = listing_df['require_guest_phone_verification'].astype('str')\n",
    "listing_df['calculated_host_listings_count'] = listing_df['calculated_host_listings_count'].astype('str')\n",
    "listing_df['calculated_host_listings_count_entire_homes'] = listing_df['calculated_host_listings_count_entire_homes'].astype('str')\n",
    "listing_df['calculated_host_listings_count_private_rooms'] = listing_df['calculated_host_listings_count_private_rooms'].astype('str')\n",
    "listing_df['calculated_host_listings_count_shared_rooms'] = listing_df['calculated_host_listings_count_shared_rooms'].astype('str')\n",
    "listing_df['reviews_per_month'] = listing_df['reviews_per_month'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# #just a check to debug cell above\n",
    "# print(listing_df['review_scores_communication'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#creating spark dataframes and passing the schemas to impart data type, exept for listing which converts from a pandas df\n",
    "df_hoods = spark.read.csv(hoods_csv, header=True, sep=\",\", schema=hoods_schema)\n",
    "df_reviews = spark.read.csv(reviews_csv, header=True, sep=\",\", schema=reviews_schema)\n",
    "df_reviews = df_reviews.withColumnRenamed(\"id\", \"review_id\").withColumnRenamed(\"date\", \"review_date\")\n",
    "df_listing = spark.createDataFrame(listing_df)\n",
    "df_calendar = spark.read.format('csv').option('header', 'true').option('inferSchema', 'true').option('sep', ',').load(calendar_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- listing_id: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- available: string (nullable = true)\n",
      " |-- adjusted_price: string (nullable = true)\n",
      " |-- minimum_nights: integer (nullable = true)\n",
      " |-- maximum_nights: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_calendar = df_calendar.drop('price')\n",
    "df_calendar.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|listing_id|reviews|\n",
      "+----------+-------+\n",
      "|    292864|    703|\n",
      "|    517425|    700|\n",
      "|    652366|    652|\n",
      "|    264459|    627|\n",
      "|  10103689|    598|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark can run sql queries on the fly without create or insert statements\n",
    "df_reviews.createOrReplaceTempView(\"reviews\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT listing_id, count(*) as reviews\n",
    "    FROM reviews\n",
    "    WHERE comments is not null\n",
    "    GROUP by 1\n",
    "    ORDER by reviews desc\n",
    "    LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The query below is basically my fact table, see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+--------------+--------------+--------------+-----+--------------------+---------+-------------------+-----------+--------------------+--------------------+---------------------+--------------------+-------+--------------------+---------+-------------------+---------------+--------------------+------------------+------------------+--------------------+-----------------+--------------------+--------------------+------------------+-------------------+-------------------------+--------------------+--------------------+----------------------+---------------+----------------------+----------------------------+--------+---------+------------------+---------------+------------+--------------+--------+----+--------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------+---------------+---------------+---------------+----------------+-----------------+---------------------+----------------------+-------------------+-------------------+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+-----------------+----------------+------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+-----------------+--------------------+-----+--------------+---------+-------------------+-----------+-------------+--------------------+\n",
      "|listing_id|      date|available|adjusted_price|minimum_nights|maximum_nights|   id|         listing_url|scrape_id|       last_scraped|     source|                name|         description|neighborhood_overview|         picture_url|host_id|            host_url|host_name|         host_since|  host_location|          host_about|host_response_time|host_response_rate|host_acceptance_rate|host_is_superhost|  host_thumbnail_url|    host_picture_url|host_neighbourhood|host_listings_count|host_total_listings_count|  host_verifications|host_has_profile_pic|host_identity_verified|  neighbourhood|neighbourhood_cleansed|neighbourhood_group_cleansed|latitude|longitude|     property_type|      room_type|accommodates|bathrooms_text|bedrooms|beds|           amenities|minimum_minimum_nights|maximum_minimum_nights|minimum_maximum_nights|maximum_maximum_nights|minimum_nights_avg_ntm|maximum_nights_avg_ntm|has_availability|availability_30|availability_60|availability_90|availability_365|number_of_reviews|number_of_reviews_ltm|number_of_reviews_l30d|       first_review|        last_review|review_scores_rating|review_scores_accuracy|review_scores_cleanliness|review_scores_checkin|review_scores_communication|review_scores_location|review_scores_value|          license|instant_bookable|calculated_host_listings_count|calculated_host_listings_count_entire_homes|calculated_host_listings_count_private_rooms|calculated_host_listings_count_shared_rooms|reviews_per_month|              name_y|price|minimum_nights|review_id|        review_date|reviewer_id|reviewer_name|            comments|\n",
      "+----------+----------+---------+--------------+--------------+--------------+-----+--------------------+---------+-------------------+-----------+--------------------+--------------------+---------------------+--------------------+-------+--------------------+---------+-------------------+---------------+--------------------+------------------+------------------+--------------------+-----------------+--------------------+--------------------+------------------+-------------------+-------------------------+--------------------+--------------------+----------------------+---------------+----------------------+----------------------------+--------+---------+------------------+---------------+------------+--------------+--------+----+--------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------+---------------+---------------+---------------+----------------+-----------------+---------------------+----------------------+-------------------+-------------------+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+-----------------+----------------+------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+-----------------+--------------------+-----+--------------+---------+-------------------+-----------+-------------+--------------------+\n",
      "|     26543|2022-09-16|        f|       $280.00|             1|            91|26543|https://www.airbn...|209132657|2022-09-16 00:00:00|city scrape|Helmholtzplatz Br...|Located directly ...| Vibes and full of...|https://a0.muscac...| 112675|https://www.airbn...|    Terri|2010-04-23 00:00:00|Berlin, Germany|I travel solo oft...|within a few hours|              100%|                100%|                t|https://a0.muscac...|https://a0.muscac...|   Prenzlauer Berg|                2.0|                      3.0|['phone', 'work_e...|                   t|                     t|Berlin, Germany|        Helmholtzplatz|                      Pankow|52.54419| 13.41956|Entire rental unit|Entire home/apt|           8|       2 baths|     2.0|   4|[\"Coffee maker\", ...|                     1|                    91|                    91|                    91|                    78|                    91|               t|             14|             42|             72|             347|              196|                    9|                     1|2010-08-17 00:00:00|2022-08-29 00:00:00|                   4|                     4|                        4|                    4|                          4|                     4|                  4|03/Z/RA/005604-20|               f|                             1|                                          1|                                           0|                                          0|             1.33|Helmholtzplatz Br...|274.0|             1|    80597|2010-08-17 00:00:00|     201072|     Thorsten|The host canceled...|\n",
      "+----------+----------+---------+--------------+--------------+--------------+-----+--------------------+---------+-------------------+-----------+--------------------+--------------------+---------------------+--------------------+-------+--------------------+---------+-------------------+---------------+--------------------+------------------+------------------+--------------------+-----------------+--------------------+--------------------+------------------+-------------------+-------------------------+--------------------+--------------------+----------------------+---------------+----------------------+----------------------------+--------+---------+------------------+---------------+------------+--------------+--------+----+--------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------+---------------+---------------+---------------+----------------+-----------------+---------------------+----------------------+-------------------+-------------------+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+-----------------+----------------+------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+-----------------+--------------------+-----+--------------+---------+-------------------+-----------+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_reviews.createOrReplaceTempView(\"reviews\")\n",
    "df_hoods.createOrReplaceTempView(\"hoods\")\n",
    "df_listing.createOrReplaceTempView(\"listing\")\n",
    "df_calendar.createOrReplaceTempView(\"calendar\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    from calendar \n",
    "    left join listing \n",
    "        on listing.id=calendar.listing_id\n",
    "    left join reviews \n",
    "        using (listing_id)\n",
    "    limit 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model:\n",
    "1. defining the destination bucket\n",
    "2. defining the dimension \"tables\" to upload with the select method, and viewing them to make sure there's no error\n",
    "3. creating the fact table \"Bookings\" by joining together the fact tables\n",
    "4. writing all the tables to S3 with the write method, assessing proper partitioning and name\n",
    "5. reading from S3 and creating again the tables objects \n",
    "6. quality checks that the rows retrieved match the uploaded in the next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "output_bucket = \"s3a://udacitycapstone123/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------------+-----------+-------------+--------------------+-----+\n",
      "|listing_id|review_id|        review_date|reviewer_id|reviewer_name|            comments|month|\n",
      "+----------+---------+-------------------+-----------+-------------+--------------------+-----+\n",
      "|      3176|     4283|2009-06-20 00:00:00|      21475|        Milan|excellent stay, i...|    6|\n",
      "|     22438|   218181|2011-04-05 00:00:00|     401483|    Alexandre|Javier gave us qu...|    4|\n",
      "|      3176|   134722|2010-11-07 00:00:00|     263467|       George|Britta's apartmen...|   11|\n",
      "+----------+---------+-------------------+-----------+-------------+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n",
      "+--------------------+-------------------+\n",
      "| neighbourhood_group|      neighbourhood|\n",
      "+--------------------+-------------------+\n",
      "|Charlottenburg-Wilm.|          Barstraße|\n",
      "|Charlottenburg-Wilm.|Charlottenburg Nord|\n",
      "|Charlottenburg-Wilm.|Düsseldorfer Straße|\n",
      "+--------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n",
      "+------------------+-----+----------+---------+--------------+--------------+--------------+\n",
      "|        listing_id|month|      date|available|adjusted_price|minimum_nights|maximum_nights|\n",
      "+------------------+-----+----------+---------+--------------+--------------+--------------+\n",
      "|652868795892201022|    9|2022-09-15|        f|        $85.00|             1|          1125|\n",
      "|652868795892201022|    9|2022-09-16|        f|        $85.00|             1|          1125|\n",
      "|652868795892201022|    9|2022-09-17|        f|        $85.00|             1|          1125|\n",
      "+------------------+-----+----------+---------+--------------+--------------+--------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n",
      "+----------+-----+--------------------+--------------------+---------+-----------+-------------------+---------------+-----------------+------------------+-----+--------------------+-----------------+---------------+\n",
      "|        id|month|                name|         description|  host_id|  host_name|         host_since|         source|         latitude|         longitude|price|review_scores_rating|reviews_per_month|      room_type|\n",
      "+----------+-----+--------------------+--------------------+---------+-----------+-------------------+---------------+-----------------+------------------+-----+--------------------+-----------------+---------------+\n",
      "|-132680130|    9|Kleine Auszeit? O...|Hallo ihr Lieben,...| 21708794|Familie Sek|2014-09-24 00:00:00|    city scrape|52.35765223245062|13.399097844958305| 88.0|                   0|              0.0|Entire home/apt|\n",
      "|  29077694|   10|Wohnung im Grünen...|Wohnung befindet ...|219116245|    Tilmann|2018-10-06 00:00:00|previous scrape|         52.41232|          13.20715| 36.0|                   4|             0.63|Entire home/apt|\n",
      "|  27080612|    5|Apartment with Li...|The well-equipped...|130216168|      Tommy|2017-05-14 00:00:00|    city scrape|         52.52006|          13.65956| 60.0|                   4|             2.54|Entire home/apt|\n",
      "+----------+-----+--------------------+--------------------+---------+-----------+-------------------+---------------+-----------------+------------------+-----+--------------------+-----------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#preparing the various \"tables\" before uploading to the S3 as parquet\n",
    "\n",
    "# REVIEW TABLE - extract columns to create review table\n",
    "df_reviews = df_reviews.na.drop()\n",
    "df_reviews = df_reviews.withColumn('month', month('review_date'))\n",
    "\n",
    "reviews_table = df_reviews.select('listing_id', 'review_id', 'review_date', 'reviewer_id', 'reviewer_name', 'comments', 'month')\n",
    "print(reviews_table.show(3))\n",
    "\n",
    "#NEIGHBOURHOODS - extract columns to create Hoods table\n",
    "hoods_table = df_hoods.select('neighbourhood_group', 'neighbourhood')\n",
    "print(hoods_table.show(3))\n",
    "\n",
    "#CALENDAR - extract columns to create CALENDAR table\n",
    "df_calendar = df_calendar.withColumn('month', month('date'))\n",
    "calendar_table = df_calendar.select('listing_id', 'month', 'date', 'available', 'adjusted_price', 'minimum_nights', 'maximum_nights')\n",
    "print(calendar_table.show(3))\n",
    "\n",
    "#LISTING - extract columns to create LISTING table\n",
    "df_listing = df_listing.withColumn('month', month('host_since'))\n",
    "#including only some columns in the listing table for demostration purposes and because it is very slow\n",
    "listing_table = df_listing.select('id', 'month','name','description','host_id','host_name','host_since','source','latitude','longitude','price','review_scores_rating','reviews_per_month','room_type','neighbourhood')\n",
    "print(listing_table.show(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- listing_id: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- available: string (nullable = true)\n",
      " |-- adjusted_price: string (nullable = true)\n",
      " |-- minimum_nights: integer (nullable = true)\n",
      " |-- maximum_nights: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- host_id: long (nullable = true)\n",
      " |-- host_name: string (nullable = true)\n",
      " |-- host_since: timestamp (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- review_scores_rating: long (nullable = true)\n",
      " |-- reviews_per_month: double (nullable = true)\n",
      " |-- room_type: string (nullable = true)\n",
      " |-- listing_id: integer (nullable = true)\n",
      " |-- review_id: integer (nullable = true)\n",
      " |-- review_date: timestamp (nullable = true)\n",
      " |-- reviewer_id: integer (nullable = true)\n",
      " |-- reviewer_name: string (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      "\n",
      "None\n",
      "+----------+----------+---------+--------------+--------------+--------------+-----+-----+-----+--------------------+--------------------+-------+---------+-------------------+-----------+--------+---------+-----+--------------------+-----------------+---------------+----------+---------+-------------------+-----------+-------------+--------------------+-----+\n",
      "|listing_id|      date|available|adjusted_price|minimum_nights|maximum_nights|month|   id|month|                name|         description|host_id|host_name|         host_since|     source|latitude|longitude|price|review_scores_rating|reviews_per_month|      room_type|listing_id|review_id|        review_date|reviewer_id|reviewer_name|            comments|month|\n",
      "+----------+----------+---------+--------------+--------------+--------------+-----+-----+-----+--------------------+--------------------+-------+---------+-------------------+-----------+--------+---------+-----+--------------------+-----------------+---------------+----------+---------+-------------------+-----------+-------------+--------------------+-----+\n",
      "|     26543|2022-09-16|        f|       $280.00|             1|            91|    9|26543|    4|Helmholtzplatz Br...|Located directly ...| 112675|    Terri|2010-04-23 00:00:00|city scrape|52.54419| 13.41956|274.0|                   4|             1.33|Entire home/apt|     26543|    80597|2010-08-17 00:00:00|     201072|     Thorsten|The host canceled...|    8|\n",
      "|     26543|2022-09-16|        f|       $280.00|             1|            91|    9|26543|    4|Helmholtzplatz Br...|Located directly ...| 112675|    Terri|2010-04-23 00:00:00|city scrape|52.54419| 13.41956|274.0|                   4|             1.33|Entire home/apt|     26543|   172181|2011-01-18 00:00:00|     339634|        April|The apartment loc...|    1|\n",
      "|     26543|2022-09-16|        f|       $280.00|             1|            91|    9|26543|    4|Helmholtzplatz Br...|Located directly ...| 112675|    Terri|2010-04-23 00:00:00|city scrape|52.54419| 13.41956|274.0|                   4|             1.33|Entire home/apt|     26543|   198268|2011-03-14 00:00:00|     144417| Jeanne-Marie|We loved the hoop...|    3|\n",
      "+----------+----------+---------+--------------+--------------+--------------+-----+-----+-----+--------------------+--------------------+-------+---------+-------------------+-----------+--------+---------+-----+--------------------+-----------------+---------------+----------+---------+-------------------+-----------+-------------+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n",
      "+-----+------------+--------------------+--------------------+-------+---------+-------------------+-----------+--------+---------+--------------+---------+-------------------+-----------+--------------------+-----------------+---------------+\n",
      "|   id|month_review|                name|         description|host_id|host_name|         host_since|     source|latitude|longitude|adjusted_price|review_id|        review_date|reviewer_id|review_scores_rating|reviews_per_month|      room_type|\n",
      "+-----+------------+--------------------+--------------------+-------+---------+-------------------+-----------+--------+---------+--------------+---------+-------------------+-----------+--------------------+-----------------+---------------+\n",
      "|26543|           8|Helmholtzplatz Br...|Located directly ...| 112675|    Terri|2010-04-23 00:00:00|city scrape|52.54419| 13.41956|       $280.00|    80597|2010-08-17 00:00:00|     201072|                   4|             1.33|Entire home/apt|\n",
      "|26543|           1|Helmholtzplatz Br...|Located directly ...| 112675|    Terri|2010-04-23 00:00:00|city scrape|52.54419| 13.41956|       $280.00|   172181|2011-01-18 00:00:00|     339634|                   4|             1.33|Entire home/apt|\n",
      "|26543|           3|Helmholtzplatz Br...|Located directly ...| 112675|    Terri|2010-04-23 00:00:00|city scrape|52.54419| 13.41956|       $280.00|   198268|2011-03-14 00:00:00|     144417|                   4|             1.33|Entire home/apt|\n",
      "+-----+------------+--------------------+--------------------+-------+---------+-------------------+-----------+--------+---------+--------------+---------+-------------------+-----------+--------------------+-----------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "##Creating the fact table BOOKINGS now\n",
    "df_bookings_stg = df_calendar.join(listing_table, on=[df_calendar.listing_id == listing_table.id], how='left')\n",
    "df_bookings = df_bookings_stg.join(df_reviews, on=[df_bookings_stg.listing_id == df_reviews.listing_id], how='left')\n",
    "print(df_bookings.printSchema())\n",
    "print(df_bookings.show(3))\n",
    "\n",
    "#preparing BOOKINGS before uploading to the S3 as parquet\n",
    "# extract columns to create review table\n",
    "df_bookings = df_bookings.withColumn('month_review', month('review_date'))\n",
    "#including only some columns in the listing table for demostration purposes and because it is very slow\n",
    "booking_table = df_bookings.select('id', 'month_review','name','description','host_id','host_name','host_since','source','latitude','longitude','adjusted_price','review_id','review_date',\n",
    "                                   'reviewer_id','review_scores_rating','reviews_per_month','room_type')\n",
    "print(booking_table.show(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "I could include data quality checks before writing to S3 to make sure the data is uploaded in the right format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, isnull\n",
    "\n",
    "# def apply_constraints(df, constraints):\n",
    "#     '''\n",
    "#     this function takes the spark dataframe and list of contraints\n",
    "#     and applies them to the df\n",
    "#     '''\n",
    "#     for constraint in constraints:\n",
    "#         df = df.filter(constraint)\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# #below the list of various constraints for the tables\n",
    "# from pyspark.sql.functions import col\n",
    "\n",
    "# review_constraints = [\n",
    "#     (col('listing_id') + col('review_date')).isUnique(),\n",
    "#     col('comments').isNotNull()]\n",
    "\n",
    "# hood_constraints = [\n",
    "#     col(\"neighbourhood\").isUnique(),\n",
    "#     col(\"neighbourhood_group\").isNotNull()]\n",
    "\n",
    "# calendar_constraints = [\n",
    "#     (col(\"listing_id\") + col(\"date\")).isUnique(),\n",
    "#     col(\"available\").isNotNull(),\n",
    "#     col(\"price\").isNotNull()]\n",
    "\n",
    "# listing_constraints = [\n",
    "#     col(\"host_id\").isUnique(),\n",
    "#     col(\"host_since\").isNotNull()]\n",
    "\n",
    "# booking_constraints = [\n",
    "#     col(\"review_id\").isUnique(),\n",
    "#     col(\"review_date\").isNotNull(),\n",
    "#     col(\"reviewer_id\").isNotNull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# #I can now use the function to pass the contraints to the dfs\n",
    "# reviews_table = apply_constraints(reviews_table, review_constraints)\n",
    "# hoods_table = apply_constraints(reviews_table, hood_constraints)\n",
    "# calendar_table = apply_constraints(reviews_table, calendar_constraints)\n",
    "# listing_table = apply_constraints(reviews_table, listing_constraints)\n",
    "# booking_table = apply_constraints(reviews_table, booking_constraints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Now that the tables have been checked for quality, they are ready to get uploaded to the S3 \"data lake\".\n",
    "\n",
    "Disclaimer: It can timeout!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbourhoods Table is created in the S3 bucket!\n",
      "Calendar Table is created in the S3 bucket!\n",
      "Listing Table is created in the S3 bucket!\n"
     ]
    }
   ],
   "source": [
    "# WRITING TABLES AS PARQUET TO S3\n",
    "\n",
    "# write REVIEW table to parquet files partitioned by month \n",
    "#reviews_table.write.mode('overwrite').partitionBy('month').parquet(output_bucket + 'reviews')\n",
    "# print('Review Table is created in the S3 bucket!')\n",
    "\n",
    "# write HOODS table to parquet files partitioned by neighbourhood_group\n",
    "hoods_table.write.mode('overwrite').partitionBy('neighbourhood_group').parquet(output_bucket + 'neighbourhoods')\n",
    "print('Neighbourhoods Table is created in the S3 bucket!')\n",
    "\n",
    "# write CALENDAR table to parquet files partitioned by month\n",
    "calendar_table.write.mode('overwrite').partitionBy('month').parquet(output_bucket + 'calendar')\n",
    "print('Calendar Table is created in the S3 bucket!')\n",
    "\n",
    "# write LISTING table to parquet files partitioned by month and room type\n",
    "listing_table.write.mode('overwrite').partitionBy('month', 'room_type').parquet(output_bucket + 'listing')\n",
    "print('Listing Table is created in the S3 bucket!')\n",
    "\n",
    "# write BOOKINGS (fact) table to parquet files partitioned month\n",
    "#booking_table.write.mode('overwrite').partitionBy('month_review').parquet(output_bucket + 'booking') #, 'room_type'\n",
    "# print('Booking Table is created in the S3 bucket!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**The two tables below TIME-OUT sometimes, so I isolated them here, this is not a syntax/code error (pls check the s3 to see they have been written before, they are too heavy (reviews..))**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "reviews_table.write.mode('overwrite').partitionBy('month').parquet(output_bucket + 'reviews')\n",
    "print('Review Table is created in the S3 bucket!')\n",
    "\n",
    "booking_table.write.mode('overwrite').partitionBy('month_review').parquet(output_bucket + 'booking')\n",
    "print('Booking Table is created in the S3 bucket!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The tables are now available to parse by accessing the S3 bucket like so:\n",
    "\n",
    "Disclaimer: It can timeout! (you can check the mentioned S3 bucket to see that it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#reading the tables from the S3 in order to parse, run analysis etc\n",
    "\n",
    "listing = spark.read.parquet(output_bucket + 'listing')\n",
    "calendar = spark.read.parquet(output_bucket + 'calendar')\n",
    "neighbourhoods = spark.read.parquet(output_bucket + 'neighbourhoods')\n",
    "# reviews = spark.read.parquet(output_bucket + 'reviews')\n",
    "# booking = spark.read.parquet(output_bucket + 'booking')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**The two tables below TIME-OUT sometimes, so I isolated them here, this is not a syntax/code error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "reviews = spark.read.parquet(output_bucket + 'reviews')\n",
    "booking = spark.read.parquet(output_bucket + 'booking')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Quality check to see if all the rows are retrieve after I upload as parquet and retrieve the parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get the count of rows in the dataframe\n",
    "uploaded_table = [listing_table,hoods_table,calendar_table] #'booking_table','reviews_table'\n",
    "downloaded_tables = [listing,neighbourhoods,calendar] #'booking','reviews'\n",
    "\n",
    "expected_count = []\n",
    "row_count = []\n",
    "\n",
    "for i in uploaded_table:\n",
    "    expected_count.append(i)\n",
    "    \n",
    "for i in downloaded_tables:\n",
    "    row_count.append(i)\n",
    "\n",
    "# Compare the counts\n",
    "for uploaded, downloaded in zip(uploaded_table, downloaded_tables):\n",
    "    if downloaded.count() != uploaded.count():\n",
    "        raise ValueError(\"Data is incomplete. Expected {} rows in {} but found {}\".format(uploaded.count(), uploaded, downloaded.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The quality check above shows matching number of rows by throwing no error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Finally to prove that the tables read from S3 can be used I run some SQL query via Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "neighbourhoods.createOrReplaceTempView(\"hoods\")\n",
    "listing.createOrReplaceTempView(\"listing\")\n",
    "calendar.createOrReplaceTempView(\"calendar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+---------+--------------+--------------+--------------+-----+\n",
      "|        listing_id|      date|available|adjusted_price|minimum_nights|maximum_nights|month|\n",
      "+------------------+----------+---------+--------------+--------------+--------------+-----+\n",
      "|652868795892201022|2022-09-16|        f|        $85.00|             1|          1125|    9|\n",
      "|652868795892201022|2022-09-17|        f|        $85.00|             1|          1125|    9|\n",
      "|652868795892201022|2022-09-18|        f|        $85.00|             1|          1125|    9|\n",
      "|652868795892201022|2022-09-19|        f|        $85.00|             1|          1125|    9|\n",
      "|652868795892201022|2022-09-20|        f|        $96.00|             1|          1125|    9|\n",
      "|652868795892201022|2022-09-21|        f|        $97.00|             1|          1125|    9|\n",
      "|652868795892201022|2022-09-22|        f|        $99.00|             1|          1125|    9|\n",
      "|652868795892201022|2022-09-23|        f|       $103.00|             1|          1125|    9|\n",
      "|652868795892201022|2022-09-24|        f|       $102.00|             1|          1125|    9|\n",
      "|652868795892201022|2022-09-25|        f|        $97.00|             1|          1125|    9|\n",
      "+------------------+----------+---------+--------------+--------------+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select * \n",
    "    from calendar \n",
    "    limit 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+-------+------------+-------------------+---------------+------------------+------------------+-----+--------------------+-----------------+---------------+-----+---------------+\n",
      "|    id|                name|         description|host_id|   host_name|         host_since|         source|          latitude|         longitude|price|review_scores_rating|reviews_per_month|  neighbourhood|month|      room_type|\n",
      "+------+--------------------+--------------------+-------+------------+-------------------+---------------+------------------+------------------+-----+--------------------+-----------------+---------------+-----+---------------+\n",
      "|179102|   The Special Place|Our appartment is...| 857327|        Alke|2011-07-23 00:00:00|    city scrape|          52.57051|          13.39872|280.0|                   4|             1.15|Berlin, Germany|    7|Entire home/apt|\n",
      "|180440|CITY STUDIO WEST@...|<b>The space</b><...| 864399|Roland & Sam|2011-07-25 00:00:00|previous scrape|52.498999999999995|          13.30171| 50.0|                   4|             0.23|Berlin, Germany|    7|Entire home/apt|\n",
      "|182816|Cozy 90sqm in Pre...|<b>The space</b><...| 876754|   Alexander|2011-07-27 00:00:00|    city scrape|          52.54185|          13.42415| 70.0|                   5|             0.01|               |    7|Entire home/apt|\n",
      "|183988|Luxury Ku'damm St...|make your visit t...| 882801|       Minel|2011-07-28 00:00:00|    city scrape|52.500009999999996|          13.30349| 76.0|                   4|             3.88|               |    7|Entire home/apt|\n",
      "|186663|Apartment Berlin ...|min rental time 6...| 897302|        Paul|2011-07-31 00:00:00|    city scrape|           52.4343|          13.23037|120.0|                   4|             0.12|               |    7|Entire home/apt|\n",
      "|186851|CasaKaiser, 2 roo...|You will be stayi...| 898345|     Susanne|2011-07-31 00:00:00|    city scrape|          52.49281|          13.34951| 37.0|                   4|             0.46|Berlin, Germany|    7|Entire home/apt|\n",
      "|195413|Residential floor...|<b>The space</b><...| 897302|        Paul|2011-07-31 00:00:00|    city scrape|          52.43399|          13.23025|300.0|                   4|             0.08|               |    7|Entire home/apt|\n",
      "|198239|Fanstastic Studio...|This apartment is...| 868242| Happy Vegan|2011-07-25 00:00:00|previous scrape|52.503009999999996|          13.41506| 76.0|                   4|             0.95|Berlin, Germany|    7|Entire home/apt|\n",
      "| 37554|More central+calm...|All apartments ha...| 161823|        Iván|2010-07-09 00:00:00|previous scrape|          52.50906|          13.39374| 19.0|                   4|             0.11|Berlin, Germany|    7|Entire home/apt|\n",
      "| 42809|The Berlin Artist...|A large art fille...| 186851|        Tina|2010-07-31 00:00:00|    city scrape|          52.51099|13.456420000000001|139.0|                   4|             1.33|Berlin, Germany|    7|Entire home/apt|\n",
      "+------+--------------------+--------------------+-------+------------+-------------------+---------------+------------------+------------------+-----+--------------------+-----------------+---------------+-----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select * \n",
    "    from listing \n",
    "    limit 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+\n",
      "|  neighbourhood|neighbourhood_group|\n",
      "+---------------+-------------------+\n",
      "|      Adlershof| Treptow - Köpenick|\n",
      "|Allende-Viertel| Treptow - Köpenick|\n",
      "|   Altglienicke| Treptow - Köpenick|\n",
      "| Altstadt-Kietz| Treptow - Köpenick|\n",
      "|   Alt  Treptow| Treptow - Köpenick|\n",
      "| Baumschulenweg| Treptow - Köpenick|\n",
      "|      Bohnsdorf| Treptow - Köpenick|\n",
      "|   Dammvorstadt| Treptow - Köpenick|\n",
      "|Friedrichshagen| Treptow - Köpenick|\n",
      "|         Grünau| Treptow - Köpenick|\n",
      "+---------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select * \n",
    "    from hoods \n",
    "    limit 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+\n",
      "|      room_type|num_listing|\n",
      "+---------------+-----------+\n",
      "|Entire home/apt|       9904|\n",
      "|   Private room|       6438|\n",
      "|    Shared room|        192|\n",
      "|     Hotel room|        146|\n",
      "+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select distinct room_type,\n",
    "        count(*) as num_listing \n",
    "    from listing\n",
    "    group by 1 \n",
    "    order by 2 desc \n",
    "    limit 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+\n",
      "|              month|num_listing_month|\n",
      "+-------------------+-----------------+\n",
      "|2022-09-01 00:00:00|                9|\n",
      "|2022-10-01 00:00:00|                9|\n",
      "|2022-11-01 00:00:00|                9|\n",
      "|2022-12-01 00:00:00|                9|\n",
      "+-------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select \n",
    "        date_trunc('month',calendar.date) as month,\n",
    "        count(distinct listing.id) as num_listing_month\n",
    "    from calendar \n",
    "    left join listing \n",
    "        on calendar.listing_id = listing.id\n",
    "    where date_trunc('year',calendar.date) = '2022'\n",
    "        and host_id like '89%'\n",
    "    group by 1 \n",
    "    order by 1 asc \n",
    "    limit 12\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "The data dictionary is included as separate file in the workspace or github repo you are checking.\n",
    "I also included the database ERD diagram in a png format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Project Write Up\n",
    "Questions + Answers:\n",
    "* **Clearly state the rationale for the choice of tools and technologies for the project.**\n",
    "\n",
    "The choice of tools and technologies for the project was based on the requirements and constraints of the project. Apache Spark was chosen for its ability to handle large data volumes and perform distributed processing, as well as its integration with S3. A star schema was chosen as the database design because it is a simple and effective way to organize data for querying and reporting, and the csv were already organised in a way that favoured this.\n",
    "\n",
    "* **Propose how often the data should be updated and why.**\n",
    "\n",
    "The data should be updated as often as is necessary to ensure that the dashboards are accurate and up-to-date. This will depend on the specific requirements and needs of the business. If the data is constantly changing and needs to be reflected in real-time, it may need to be updated more frequently. On the other hand, if the data only changes occasionally and the dashboard does not need to be updated in real-time, it may be sufficient to update the data less frequently, e.g. once a day.\n",
    "\n",
    "* **Write a description of how you would approach the problem differently under the following scenarios:**\n",
    "\n",
    "* **The data was increased by 100x.**\n",
    " \n",
    " If the data was increased by 100x, the current approach to the problem may not be sufficient to handle the larger data volume. One potential solution could be to use a distributed database such as Hadoop or Apache Cassandra, which are designed to handle very large data volumes. Another option could be to involve a cloud-based data warehousing solution such as Amazon Redshift, which is optimized for fast querying and analysis of large datasets.\n",
    "\n",
    " \n",
    " * **The data populates a dashboard that must be updated on a daily basis by 7am every day.**\n",
    " \n",
    " If the dashboard must be updated on a daily basis by 7am every day, it will be necessary to automate the data update process. This could involve setting up a scheduled job or workflow to load the data into the database on a daily basis. It may also be necessary to optimize the data loading process to ensure that it can be completed within the required time frame.\n",
    " Examples of tools that can be used to update the dashboards on a daily basis:\n",
    " -Workflow orchestration tools: Apache Airflow can be used to define and schedule data pipelines that include tasks such as data loading, transformation, and visualization.\n",
    " -Cloud-based schedulers: Cloud providers offer scheduling services that can be used to run jobs on a regular basis. Example, Amazon Web Services (AWS) offers the Simple Scheduler for EC2 instances.\n",
    " \n",
    " \n",
    " * **The database needed to be accessed by 100+ people.**\n",
    " \n",
    " If the database needed to be accessed by 100+ people, it may be necessary to scale up the infrastructure to support the increased demand. This could involve using a higher-capacity database server or a distributed database system that can scale horizontally to support more concurrent users. It may also be necessary to implement additional security measures to ensure that the database is protected against unauthorized access.\n",
    " \n",
    " Here are some tools that you could use to scale up the infrastructure and support more concurrent users:\n",
    "-Cloud data warehouses: Amazon Redshift, Google BigQuery, Snowflake\n",
    "-Relational databases: PostgreSQL, MySQL\n",
    "-NoSQL databases: Apache Cassandra, MongoDB, Google Cloud Bigtable\n",
    "-Data lakes: Amazon S3, Azure Data Lake Storage\n",
    "-Big data processing frameworks: Apache Spark, Apache Hadoop\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
